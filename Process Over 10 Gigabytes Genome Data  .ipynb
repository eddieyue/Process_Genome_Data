{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Genome dataset from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import DAL\n",
    "import numpy as np \n",
    "from numpy import *\n",
    "\n",
    "genomes = DAL.create(\"genomes\")\n",
    "genome_list = genomes.subsets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up multiple working clusters by Ipython.parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "\n",
    "rc = Client()\n",
    "\n",
    "import time\n",
    "while len(rc) < 4: \n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "dview = rc[:]\n",
    "dview.retries = 10\n",
    "\n",
    "for i in rc.ids:\n",
    "    rc[i][\"id\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a strand file for each genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "@dview.remote(block=True)\n",
    "def generate_strandfile():\n",
    "    import DAL\n",
    "    genomes = DAL.create(\"genomes\")\n",
    "    strandfile_ref=[]\n",
    "    for genome_name in genome_list:\n",
    "        fp = open(\"/tmp/strand_file.fa\"+str(genome_list.index(genome_name))+str(id), \"w\")\n",
    "        strandfile_ref.append(DAL.helpers.FileReference(fp))\n",
    "        for chunk in genomes.k_mers(genome_name,20):\n",
    "            fp.write(chunk+\" \"+genome_name+\"\\n\")\n",
    "        fp.close\n",
    "    return strandfile_ref       \n",
    "    \n",
    "dview.scatter(\"genome_list\", genome_list)\n",
    "\n",
    "temp = generate_strandfile()\n",
    "\n",
    "strandfile_ref = []\n",
    "for sublist in temp:\n",
    "    for item in sublist:\n",
    "        strandfile_ref.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#As the size of the strandfile is too large for the single working cluster's memory, I split the strand file to 1,000,000-row chunks. Then I could process every chunk once, save the result into disk and free the memory.# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "splitLen = 1000000   \n",
    "dview[\"splitLen\"] = splitLen\n",
    "@dview.remote(block=True) \n",
    "def sortedfile_ref():\n",
    "    ssortedfile_ref = []\n",
    "    import DAL\n",
    "    import heapq\n",
    "    from heapq import merge\n",
    "    for file_ref in strandfile_ref:\n",
    "        sorted_file = []\n",
    "        temp_file = file_ref.open(\"r\")\n",
    "        input_data = temp_file.readlines()\n",
    "        temp_file.close\n",
    "        file_ref.remove_local_copy()\n",
    "        ttempfile_ref = []\n",
    "        \n",
    "        lenrange = range(0, len(input_data), splitLen)\n",
    "        for start_index in lenrange:\n",
    "            if start_index+splitLen <= len(input_data):\n",
    "                end_index = start_index + splitLen\n",
    "                output_data = input_data[start_index:end_index]\n",
    "            else:\n",
    "                output_data = input_data[start_index:-1]\n",
    "                \n",
    "            output_data.sort()\n",
    "            ttemp_file = open(file_ref.filename+\".tmp\" +str(lenrange.index(start_index)), \"w\")\n",
    "            ttempfile_ref.append(DAL.helpers.FileReference(ttemp_file))\n",
    "            \n",
    "            for lines in output_data:\n",
    "                ttemp_file.write(lines)\n",
    "            ttemp_file.close\n",
    "            \n",
    "            \n",
    "        sortedtemp_file = []    \n",
    "     \n",
    "    \n",
    "        for ffile_ref in ttempfile_ref:\n",
    "            sortedtemp_file.append(ffile_ref.open(\"r\"))\n",
    "     \n",
    "        with open(file_ref.filename+\".sorted\", \"w\") as r:\n",
    "            ssortedfile_ref.append(DAL.helpers.FileReference(r))\n",
    "            for chunk in merge(*sortedtemp_file):\n",
    "                r.write(chunk)  \n",
    "        \n",
    "        for ffile_ref in ttempfile_ref:\n",
    "            ffile_ref.remove_local_copy()\n",
    "        \n",
    "        \n",
    "        for tttemp_file in sortedtemp_file:\n",
    "            tttemp_file.close()\n",
    "   \n",
    "            \n",
    "    ssortedtemp_file = [] \n",
    "    for fffile_ref in ssortedfile_ref:\n",
    "        ssortedtemp_file.append(fffile_ref.open(\"r\"))\n",
    "    \n",
    "    with open(\"/tmp/submerge_file.fa\" + str(id), \"w\" ) as m:\n",
    "        submergedfile_ref = DAL.helpers.FileReference(m)\n",
    "        for cchunk in merge(*ssortedtemp_file):\n",
    "            m.write(cchunk)\n",
    "            \n",
    "    for fffile_ref in ssortedfile_ref:\n",
    "        fffile_ref.remove_local_copy()\n",
    "            \n",
    "    for ttttemp_file in ssortedtemp_file:\n",
    "            ttttemp_file.close()\n",
    "    \n",
    "      \n",
    "    return submergedfile_ref \n",
    "     \n",
    "dview.scatter(\"strandfile_ref\", strandfile_ref)\n",
    "sortedfile_ref = sortedfile_ref()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I am going to combine all the strand files to one file by merge in the heapq module due to the limitation of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n",
    "from heapq import merge\n",
    "temp_file = []\n",
    "for file_ref in sortedfile_ref:\n",
    "    temp_file.append(file_ref.open(\"r\"))\n",
    "\n",
    "with open(\"/tmp/merged_file.fa\", \"w\" ) as f:\n",
    "        mergedfile_ref = DAL.helpers.FileReference(f)\n",
    "        for chunk in merge(*temp_file):\n",
    "            f.write(chunk)    \n",
    "\n",
    "for file_ref in sortedfile_ref:\n",
    "    file_ref.remove_local_copy()\n",
    "            \n",
    "for ttemp_file in temp_file:\n",
    "    ttemp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print first few lines to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAAAAAAAAAAAATTACCCC L._Ipsum-strain00.fa\\n'\n",
      " 'AAAAAAAAAAAAATTACCCC L._Ipsum-strain03.fa\\n'\n",
      " 'AAAAAAAAAAAATTACCCCT L._Ipsum-strain00.fa\\n'\n",
      " 'AAAAAAAAAAAATTACCCCT L._Ipsum-strain03.fa\\n'\n",
      " 'AAAAAAAAAAAGATTACCCC L._Ipsum-strain02.fa\\n'\n",
      " 'AAAAAAAAAAAGGAAACTTA L._Ipsum-strain01.fa\\n'\n",
      " 'AAAAAAAAAAAGGGACTAGC L._Ipsum-strain02.fa\\n'\n",
      " 'AAAAAAAAAAAGGGACTAGG L._Ipsum-strain00.fa\\n'\n",
      " 'AAAAAAAAAAAGGGACTAGG L._Ipsum-strain03.fa\\n'\n",
      " 'AAAAAAAAAAAGTACATAGC L._Ipsum-strain03.fa\\n']\n"
     ]
    }
   ],
   "source": [
    "temp_file = mergedfile_ref.open(\"r\")\n",
    "input_data = temp_file.readlines()\n",
    "temp_file.close()\n",
    "mergedfile_ref.remove_local_copy()\n",
    "print array(input_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#function to split the all_chunk to smaller files. And without split the strand has same genome into two files with the size 400,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitLen = 400000 \n",
    "\n",
    "start_index = 0\n",
    "end_index = start_index + splitLen\n",
    "count = 0\n",
    "temp_ref = []\n",
    "\n",
    "\n",
    "while end_index <= len(input_data):\n",
    "    \n",
    "    \n",
    "    if input_data[end_index-1][:20] ==  input_data[end_index][:20]:\n",
    "        end_index += 1\n",
    "    \n",
    "    output_data = input_data[start_index:end_index]\n",
    "    with open(\"/tmp/temp_all%s.fa\" %count, \"w\") as r:\n",
    "        temp_ref.append(DAL.helpers.FileReference(r))\n",
    "        for lines in output_data:\n",
    "            r.write(lines)\n",
    "    count += 1\n",
    "    start_index = end_index\n",
    "    end_index = start_index + splitLen\n",
    "\n",
    "    \n",
    "output_data = input_data[start_index:-1]\n",
    "with open(\"/tmp/temp_all%s.fa\" %count, \"w\") as r:\n",
    "    temp_ref.append(DAL.helpers.FileReference(r))\n",
    "    for lines in output_data:\n",
    "        r.write(lines)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to find-out the similarity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sim_mat(x):    ## input argument would be a reference\n",
    "    import DAL\n",
    "    import numpy as np\n",
    "    simmat = np.zeros((n,n))\n",
    "    simmat = simmat.tolist()\n",
    "    temp = x.open(\"r\")\n",
    "    input_data = temp.readlines()\n",
    "    temp.close()\n",
    "    start_index = 0\n",
    "    end_index = start_index + 1\n",
    "    while end_index < len(input_data):\n",
    "        [temp_genome,temp_strand] = input_data[start_index].split(\" \")\n",
    "        [test_genome,test_strand] = input_data[end_index].split(\" \")\n",
    "        row_index = int(''.join(ele for ele in temp_strand if ele.isdigit()))\n",
    "    \n",
    "        while temp_genome == test_genome:\n",
    "            col_index = int(''.join(ele for ele in test_strand if ele.isdigit()))\n",
    "            if col_index != row_index:\n",
    "                simmat[row_index][col_index] += 1\n",
    "                simmat[col_index][row_index] += 1\n",
    "            end_index += 1\n",
    "            if end_index >= len(input_data):\n",
    "                break\n",
    "            [test_genome,test_strand] = input_data[end_index].split(\" \")\n",
    "        start_index = end_index\n",
    "        end_index = start_index + 1\n",
    "    return np.array(simmat)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out the Similarity_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my Similarity-Matrix:\n",
      "\n",
      "[[       0.  1709899.  1713976.  2426326.]\n",
      " [ 1709899.        0.  1490516.   866287.]\n",
      " [ 1713976.  1490516.        0.   457548.]\n",
      " [ 2426326.   866287.   457548.        0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(genome_list)\n",
    "\n",
    "dview[\"n\"] = n\n",
    "simmat = sum(dview.map_sync(sim_mat,temp_ref), axis=0)\n",
    "\n",
    "print \"Here is my Similarity-Matrix:\" + \"\\n\"\n",
    "print simmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For this part, I would firstly gourp the genomes with the maximum similarity. The maximum similarity represents the smallest DNA distance, which means the two genomes must be closely realated. After merge the pair of genome has the maximum similarity, I need to update other elements in the sorted list. \n",
    "\n",
    "#For example, I merge pair (0,3), for other pair like (0,1), (3,1). I would choose the pair has better similarity to update, for example if (0,1) > (3,1), new((0,1)) = old((0,1)). Base on my understanding, the more ancient spices would have better similar to all other spices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is sorted list of pair of genomes :\n",
      "\n",
      "[[0, 3], [0, 2], [0, 1], [1, 2], [1, 3], [2, 3]]\n"
     ]
    }
   ],
   "source": [
    "temp = zeros((n,n))\n",
    "temp = triu(simmat)[:]\n",
    "ttemp = temp[np.nonzero(temp)]\n",
    "tttemp = sorted(ttemp, reverse = True)\n",
    "result = [np.argwhere(temp == elements) for elements in tttemp]\n",
    "pair_list = [ele[0].tolist() for ele in result]\n",
    "\n",
    "print \"Here is sorted list of pair of genomes :\" + \"\\n\"\n",
    "print pair_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "temp_pair = zip(tttemp,pair_list )\n",
    "target = temp_pair[0][1]\n",
    "events = []\n",
    "while len(temp_pair) > 3:\n",
    "    \n",
    "    \n",
    "        \n",
    "    temp_seq = range(n)\n",
    "    target = temp_pair[0][1]\n",
    "    [a,b]  = target\n",
    "    c = min(target)\n",
    "\n",
    "    ttemp_seq = temp_seq[:]\n",
    "    ttemp_seq.pop(ttemp_seq.index(a))\n",
    "    ttemp_seq.pop(ttemp_seq.index(b))\n",
    "\n",
    "\n",
    "\n",
    "    keeppair_list=[]\n",
    "    for ele in pair_list:\n",
    "        [s,t] = ele\n",
    "        if s != a and s != b and t != a and t != b:\n",
    "            keeppair_list.append(ele)\n",
    "    \n",
    "    \n",
    "    tempkeeppair_list = []        \n",
    "    for tele in keeppair_list:\n",
    "        for ttele in temp_pair:\n",
    "            [g,h] = ttele\n",
    "            if h == tele:\n",
    "                tempkeeppair_list.append(ttele)\n",
    "                           \n",
    "    for num in ttemp_seq:\n",
    "        test_list = [[a,num],[num,a],[num,b],[b,num]]\n",
    "        ttest_list = []\n",
    "        for coord in test_list:\n",
    "            ttest_list.append([o for o,p in temp_pair if p == coord])\n",
    "\n",
    "        temp_index = argmax(ttest_list)\n",
    "        target_pair = test_list[argmax(ttest_list)]\n",
    "        keep_number = temp_pair[pair_list.index(target_pair)][0]\n",
    "        keep_pair = [c, num]\n",
    "        keeppair_list.append(keep_pair)\n",
    "        tempkeep_pair = (keep_number, keep_pair)\n",
    "        tempkeeppair_list.append(tempkeep_pair)\n",
    "\n",
    "\n",
    "    temp_pair = sorted(tempkeeppair_list, key=lambda tempkeeppair: tempkeeppair[0],reverse = True)\n",
    "    pair_list = [j for i,j in temp_pair]\n",
    "    events.append(\"strain%s branched into strain%s and strain%s \" %(c, a, b))\n",
    "    \n",
    "if len(temp_pair) == 3:\n",
    "    target = temp_pair[0][1]\n",
    "    [a,b]  = target\n",
    "    c = min(target)\n",
    "    tempkeeppair_list = []\n",
    "    (e,f)=temp_pair[1]\n",
    "    for nnum in f:\n",
    "        if nnum != a and nnum != b: \n",
    "            tempkeeppair_list.append((e,[c, nnum]))\n",
    "    temp_pair = sorted(tempkeeppair_list, key=lambda tempkeeppair: tempkeeppair[0],reverse = True)\n",
    "    pair_list = [j for i,j in temp_pair]\n",
    "    events.append(\"strain%s branched into strain%s and strain%s \" %(c, a, b))\n",
    "\n",
    "if len(temp_pair) == 1:\n",
    "    target = temp_pair[0][1]\n",
    "    [a,b]  = target\n",
    "    c = min(target)\n",
    "    events.append(\"strain%s branched into strain%s and strain%s \" %(c, a, b))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out the merge events of evolution in order to reconstruct the Cladogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strain0 branched into strain0 and strain1 \n",
      "strain0 branched into strain0 and strain2 \n",
      "strain0 branched into strain0 and strain3 \n"
     ]
    }
   ],
   "source": [
    "for log in events[::-1]:\n",
    "    print log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the record of grouping the genomes, as the we are grouping the genome from the lower level to higher level. Therefore, we shuold print out all events in reverse, which would be in chronological order. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
